{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6145331",
   "metadata": {},
   "source": [
    "# Grid World Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e4b4c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 3. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 1, Reward: -1\n",
      "[[ 3.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 0, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 3. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 1, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 3.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 1, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 3.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 2, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 3.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 2, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 3.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 1, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 3.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 2, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 3.  0.  0.  0.  2.]]\n",
      "Action: 1, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  3.  0.  0.  2.]]\n",
      "Action: 3, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  3.  0.  2.]]\n",
      "Action: 3, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  3.  2.]]\n",
      "Action: 3, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  3.]]\n",
      "Action: 3, Reward: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, grid_size, start, goal, obstacles):\n",
    "        super(GridWorldEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles\n",
    "        self.action_space = spaces.Discrete(4)  # Four actions: up, down, left, right\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(grid_size[0]),\n",
    "            spaces.Discrete(grid_size[1])\n",
    "        ))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = list(self.start)\n",
    "        return tuple(self.agent_pos)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Define actions: 0=up, 1=down, 2=left, 3=right\n",
    "        if action == 0 and self.agent_pos[0] > 0:  # Up\n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 1 and self.agent_pos[0] < self.grid_size[0] - 1:  # Down\n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 2 and self.agent_pos[1] > 0:  # Left\n",
    "            self.agent_pos[1] -= 1\n",
    "        elif action == 3 and self.agent_pos[1] < self.grid_size[1] - 1:  # Right\n",
    "            self.agent_pos[1] += 1\n",
    "        \n",
    "        reward = -1\n",
    "        done = False\n",
    "        if tuple(self.agent_pos) == self.goal:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif tuple(self.agent_pos) in self.obstacles:\n",
    "            reward = -10\n",
    "            done = True\n",
    "\n",
    "        return tuple(self.agent_pos), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.zeros(self.grid_size)\n",
    "        grid[self.start] = 1  # Start position\n",
    "        grid[self.goal] = 2  # Goal position\n",
    "        for obstacle in self.obstacles:\n",
    "            grid[obstacle] = -1  # Obstacle positions\n",
    "        grid[tuple(self.agent_pos)] = 3  # Agent position\n",
    "        print(grid)\n",
    "\n",
    "# Example usage\n",
    "grid_size = (5, 5)\n",
    "start = (0, 0)\n",
    "goal = (4, 4)\n",
    "obstacles = [(1, 1), (2, 2), (3, 3)]\n",
    "\n",
    "env = GridWorldEnv(grid_size, start, goal, obstacles)\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614eee2c",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "caa88e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 5, Reward: 0.40373590725149966\n",
      "Action: 6, Reward: -1.5682592517119807\n",
      "Action: 9, Reward: 1.7101819968807335\n",
      "Action: 3, Reward: -0.02057601668038067\n",
      "Action: 1, Reward: 0.8522818793842781\n",
      "Action: 0, Reward: -0.6532468191367629\n",
      "Action: 2, Reward: 0.8926445934082499\n",
      "Action: 3, Reward: 2.5665910308669995\n",
      "Action: 8, Reward: 0.3812166853629977\n",
      "Action: 2, Reward: 0.5895597532453235\n",
      "Action: 0, Reward: -1.535064337854422\n",
      "Action: 6, Reward: -0.16983353715885718\n",
      "Action: 6, Reward: -0.08292468759490379\n",
      "Action: 6, Reward: -0.14443598378826272\n",
      "Action: 4, Reward: -0.899093203150997\n",
      "Action: 2, Reward: 1.6113902442569712\n",
      "Action: 9, Reward: 0.4869177290045204\n",
      "Action: 4, Reward: -0.18073610705549026\n",
      "Action: 4, Reward: 0.26995636804059\n",
      "Action: 5, Reward: 0.15240136142502553\n",
      "Action: 8, Reward: 2.0155883483962693\n",
      "Action: 2, Reward: 2.116799243819492\n",
      "Action: 3, Reward: 0.6936743554941673\n",
      "Action: 9, Reward: 0.3186059913055259\n",
      "Action: 0, Reward: -1.2369020922824094\n",
      "Action: 8, Reward: -0.37756638257541697\n",
      "Action: 1, Reward: -0.10979224323596914\n",
      "Action: 8, Reward: 0.24320579345068816\n",
      "Action: 5, Reward: 1.5156551763932835\n",
      "Action: 2, Reward: -0.31071305005204897\n",
      "Action: 9, Reward: 0.8311894144013153\n",
      "Action: 4, Reward: -1.6431441288849262\n",
      "Action: 9, Reward: 0.6700570154102329\n",
      "Action: 4, Reward: -0.12646495987788298\n",
      "Action: 5, Reward: 1.8842060284699715\n",
      "Action: 3, Reward: 1.0763574112683119\n",
      "Action: 3, Reward: 1.0781626992571032\n",
      "Action: 2, Reward: -0.04484765814793082\n",
      "Action: 3, Reward: 1.2006010085948926\n",
      "Action: 2, Reward: -0.07990129587571076\n",
      "Action: 0, Reward: -1.54212498778795\n",
      "Action: 8, Reward: 0.003142089990715413\n",
      "Action: 6, Reward: -0.8966398364422146\n",
      "Action: 8, Reward: 2.5877471473661684\n",
      "Action: 5, Reward: -1.9295295579995078\n",
      "Action: 1, Reward: 2.0512351715719044\n",
      "Action: 8, Reward: 0.5571700109492795\n",
      "Action: 0, Reward: 1.54865340370809\n",
      "Action: 0, Reward: -0.6136736034611906\n",
      "Action: 3, Reward: 2.967533603272253\n",
      "Action: 9, Reward: 0.6944541566201702\n",
      "Action: 5, Reward: 0.16986445802339795\n",
      "Action: 0, Reward: -1.1159127026069844\n",
      "Action: 7, Reward: 0.6201273778348104\n",
      "Action: 7, Reward: 0.4094173307839215\n",
      "Action: 4, Reward: 0.22182423523044548\n",
      "Action: 6, Reward: 1.590775276483661\n",
      "Action: 4, Reward: 0.0037752191284625425\n",
      "Action: 2, Reward: 0.25975318979448864\n",
      "Action: 4, Reward: -0.29296793124945186\n",
      "Action: 1, Reward: 1.2440581019232508\n",
      "Action: 7, Reward: 0.7557370538756588\n",
      "Action: 4, Reward: -1.1587729215684472\n",
      "Action: 2, Reward: -0.25279059589928865\n",
      "Action: 2, Reward: -0.4054152461989641\n",
      "Action: 0, Reward: 2.145767087557008\n",
      "Action: 0, Reward: 1.4616294318166045\n",
      "Action: 8, Reward: 1.3083294369891656\n",
      "Action: 1, Reward: 1.6390012438198425\n",
      "Action: 9, Reward: -1.5893606848192419\n",
      "Action: 8, Reward: 0.9335689533943659\n",
      "Action: 0, Reward: 1.322153348480775\n",
      "Action: 6, Reward: -0.23541419601518565\n",
      "Action: 2, Reward: -0.4182128154309015\n",
      "Action: 9, Reward: -0.5935101609836215\n",
      "Action: 5, Reward: 0.1154034382755007\n",
      "Action: 0, Reward: 0.7237425703050929\n",
      "Action: 0, Reward: 0.5342865659528864\n",
      "Action: 8, Reward: 0.8535389814099488\n",
      "Action: 8, Reward: 1.18313270146414\n",
      "Action: 4, Reward: 0.865387060931298\n",
      "Action: 1, Reward: 0.15740998502973175\n",
      "Action: 2, Reward: 1.3364338153060604\n",
      "Action: 4, Reward: 0.4880451353764226\n",
      "Action: 6, Reward: -0.18714924591042448\n",
      "Action: 2, Reward: 1.6754123311341251\n",
      "Action: 6, Reward: 0.31319598386720987\n",
      "Action: 3, Reward: 1.077814043050749\n",
      "Action: 5, Reward: 0.02580286722030481\n",
      "Action: 4, Reward: -0.5383869329532832\n",
      "Action: 8, Reward: 0.6359212699681249\n",
      "Action: 7, Reward: -0.09309470783558677\n",
      "Action: 3, Reward: 1.9194719930700135\n",
      "Action: 0, Reward: 0.2266757654439043\n",
      "Action: 0, Reward: -0.8309626578341619\n",
      "Action: 7, Reward: -1.0834916326816901\n",
      "Action: 5, Reward: 1.8438268495788588\n",
      "Action: 3, Reward: 0.6527862108914926\n",
      "Action: 8, Reward: 2.0716636869407026\n",
      "Action: 4, Reward: 0.7516067570932192\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiArmedBanditEnv(gym.Env):\n",
    "    def __init__(self, k_arms, reward_distributions):\n",
    "        super(MultiArmedBanditEnv, self).__init__()\n",
    "        self.k_arms = k_arms\n",
    "        self.reward_distributions = reward_distributions\n",
    "        self.action_space = spaces.Discrete(k_arms)\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        return 0  # Single-state problem\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = np.random.normal(self.reward_distributions[action][0], self.reward_distributions[action][1])\n",
    "        return 0, reward, False, {}  # Single-state, so always return state 0\n",
    "\n",
    "# Example usage\n",
    "k_arms = 10\n",
    "reward_distributions = [(np.random.rand(), 1) for _ in range(k_arms)]\n",
    "\n",
    "env = MultiArmedBanditEnv(k_arms, reward_distributions)\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa82cf",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b43af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    n_states = env.observation_space[0].n * env.observation_space[1].n\n",
    "    value_table = np.zeros(n_states)\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(n_states):\n",
    "            v = value_table[state]\n",
    "            q_values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                prob = 1  # deterministic environment\n",
    "                q_values.append(prob * (reward + gamma * value_table[next_state[0] * env.grid_size[1] + next_state[1]]))\n",
    "            value_table[state] = max(q_values)\n",
    "            delta = max(delta, abs(v - value_table[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    for state in range(n_states):\n",
    "        q_values = []\n",
    "        for action in range(env.action_space.n):\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            prob = 1  # deterministic environment\n",
    "            q_values.append(prob * (reward + gamma * value_table[next_state[0] * env.grid_size[1] + next_state[1]]))\n",
    "        policy[state] = np.argmax(q_values)\n",
    "\n",
    "    return policy, value_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d0d91",
   "metadata": {},
   "source": [
    "# Policy Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bafa1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    n_states = env.observation_space[0].n * env.observation_space[1].n\n",
    "    n_actions = env.action_space.n\n",
    "    policy = np.random.choice(n_actions, n_states)\n",
    "    value_table = np.zeros(n_states)\n",
    "\n",
    "    def one_step_lookahead(state, value_table):\n",
    "        action_values = np.zeros(n_actions)\n",
    "        for action in range(n_actions):\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            prob = 1  # deterministic environment\n",
    "            next_state_index = next_state[0] * env.grid_size[1] + next_state[1]\n",
    "            action_values[action] += prob * (reward + gamma * value_table[next_state_index])\n",
    "        return action_values\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(n_states):\n",
    "                v = value_table[state]\n",
    "                action = policy[state]\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state_index = next_state[0] * env.grid_size[1] + next_state[1]\n",
    "                value_table[state] = reward + gamma * value_table[next_state_index]\n",
    "                delta = max(delta, abs(v - value_table[state]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for state in range(n_states):\n",
    "            chosen_action = policy[state]\n",
    "            action_values = one_step_lookahead(state, value_table)\n",
    "            best_action = np.argmax(action_values)\n",
    "            if chosen_action != best_action:\n",
    "                policy_stable = False\n",
    "            policy[state] = best_action\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, value_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413d845",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc80eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=1000):\n",
    "    n_states = env.observation_space[0].n * env.observation_space[1].n\n",
    "    n_actions = env.action_space.n\n",
    "    q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "    def choose_action(state):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return np.random.choice(n_actions)\n",
    "        else:\n",
    "            return np.argmax(q_table[state, :])\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        state_index = state[0] * env.grid_size[1] + state[1]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = choose_action(state_index)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state_index = next_state[0] * env.grid_size[1] + next_state[1]\n",
    "            q_table[state_index, action] += alpha * (reward + gamma * np.max(q_table[next_state_index, :]) - q_table[state_index, action])\n",
    "            state_index = next_state_index\n",
    "\n",
    "    policy = np.argmax(q_table, axis=1)\n",
    "    return policy, q_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2aa894",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy Policy for Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dae443ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_bandit(env, alpha=0.1, epsilon=0.1, episodes=1000):\n",
    "    q_values = np.zeros(env.action_space.n)\n",
    "    action_counts = np.zeros(env.action_space.n)\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        action_counts[action] += 1\n",
    "        q_values[action] += (reward - q_values[action]) / action_counts[action]\n",
    "\n",
    "    return q_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd33c40",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound (UCB) Algorithm for Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48f62d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_bandit(env, alpha=0.1, episodes=1000):\n",
    "    q_values = np.zeros(env.action_space.n)\n",
    "    action_counts = np.zeros(env.action_space.n)\n",
    "    total_steps = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        total_steps += 1\n",
    "        ucb_values = q_values + np.sqrt(2 * np.log(total_steps) / (action_counts + 1e-5))\n",
    "        action = np.argmax(ucb_values)\n",
    "\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        action_counts[action] += 1\n",
    "        q_values[action] += (reward - q_values[action]) / action_counts[action]\n",
    "\n",
    "    return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee2a70a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Value Iteration...\n",
      "Optimal Policy (Value Iteration):\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "Value Table (Value Iteration):\n",
      "[[-99.99990137 -99.99990137 -99.99990137 -99.99990137 -99.99990137]\n",
      " [-99.99990137 -99.99990235 -99.99990235 -99.99990235 -99.99990235]\n",
      " [-99.99990235 -99.99990235 -99.99990235 -99.99990235 -99.99990235]\n",
      " [-99.99990235 -99.99990235 -99.99990235 -99.99990235 -99.99990235]\n",
      " [-99.99990235 -99.99990235 -99.99990235 -99.99990235 -99.99990235]]\n",
      "\n",
      "Testing Policy Iteration...\n",
      "Optimal Policy (Policy Iteration):\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "Value Table (Policy Iteration):\n",
      "[[-100. -100. -100. -100. -100.]\n",
      " [-100. -100. -100. -100. -100.]\n",
      " [-100. -100. -100. -100. -100.]\n",
      " [-100. -100. -100. -100. -100.]\n",
      " [-100. -100. -100. -100. -100.]]\n",
      "\n",
      "Testing Q-Learning...\n",
      "Optimal Policy (Q-Learning):\n",
      "[[3 3 3 1 1]\n",
      " [1 0 3 3 1]\n",
      " [3 1 0 3 1]\n",
      " [3 3 1 0 1]\n",
      " [3 3 3 3 0]]\n",
      "Q-Table (Q-Learning):\n",
      "[[[ 0.83677743 -2.78965913  0.5753928   2.52718827]\n",
      "  [ 1.57334445 -9.47665237  1.05743352  3.56281643]\n",
      "  [ 3.00582123  2.65663989  2.18478496  4.60890549]\n",
      "  [ 2.62260813  5.6655611   2.81817182  4.9865626 ]\n",
      "  [ 0.73318466  6.70344024 -0.33963284 -0.98188838]]\n",
      "\n",
      " [[-2.52567923 -1.00203824 -3.06282656 -3.439     ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-1.62859697 -3.439      -1.9         5.43741943]\n",
      "  [ 4.10040223  5.11723668  2.2975238   6.73289   ]\n",
      "  [ 4.61734603  7.811       5.15674538  5.8284129 ]]\n",
      "\n",
      " [[-2.74584973 -2.03225145 -2.12012253  1.21420519]\n",
      "  [-3.439       3.57868391 -1.3508746  -3.439     ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.14371986 -1.         -1.          7.76381411]\n",
      "  [ 5.85666661  8.9         5.93953612  7.09451185]]\n",
      "\n",
      " [[-1.36202852 -1.34178779 -1.36965278 -0.56543139]\n",
      "  [-0.36518036 -0.6781213  -1.02031956  5.55737179]\n",
      "  [-2.71        7.33629873  0.08459017 -1.9       ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 7.18815541 10.         -9.28210201  8.32495102]]\n",
      "\n",
      " [[-0.98305159 -0.89640839 -0.89640839 -0.66708054]\n",
      "  [-0.52521216 -0.499001   -0.549225    2.08420221]\n",
      "  [-0.27046375 -0.1999     -0.25758377  8.77904296]\n",
      "  [-2.71        2.99258946  0.73525196  9.98003322]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_grid_world_algorithms():\n",
    "    # Define the Grid World environment\n",
    "    grid_size = (5, 5)\n",
    "    start = (0, 0)\n",
    "    goal = (4, 4)\n",
    "    obstacles = [(1, 1), (2, 2), (3, 3)]\n",
    "    \n",
    "    env = GridWorldEnv(grid_size, start, goal, obstacles)\n",
    "\n",
    "    # Test Value Iteration\n",
    "    print(\"Testing Value Iteration...\")\n",
    "    policy, value_table = value_iteration(env)\n",
    "    print(\"Optimal Policy (Value Iteration):\")\n",
    "    print(policy.reshape(grid_size))\n",
    "    print(\"Value Table (Value Iteration):\")\n",
    "    print(value_table.reshape(grid_size))\n",
    "    print()\n",
    "\n",
    "    # Test Policy Iteration\n",
    "    print(\"Testing Policy Iteration...\")\n",
    "    policy, value_table = policy_iteration(env)\n",
    "    print(\"Optimal Policy (Policy Iteration):\")\n",
    "    print(policy.reshape(grid_size))\n",
    "    print(\"Value Table (Policy Iteration):\")\n",
    "    print(value_table.reshape(grid_size))\n",
    "    print()\n",
    "\n",
    "    # Test Q-Learning\n",
    "    print(\"Testing Q-Learning...\")\n",
    "    policy, q_table = q_learning(env)\n",
    "    print(\"Optimal Policy (Q-Learning):\")\n",
    "    print(policy.reshape(grid_size))\n",
    "    print(\"Q-Table (Q-Learning):\")\n",
    "    print(q_table.reshape(grid_size[0], grid_size[1], -1))\n",
    "    print()\n",
    "\n",
    "# Run the test function\n",
    "test_grid_world_algorithms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fa9d56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Epsilon-Greedy Policy...\n",
      "Estimated Q-Values (Epsilon-Greedy):\n",
      "[ 0.65966084  0.47458021  0.838381   -0.03847589 -0.00824702  0.50346637\n",
      "  0.65733079  0.88689306 -0.05956219  0.63702932]\n",
      "\n",
      "Testing UCB Algorithm...\n",
      "Estimated Q-Values (UCB):\n",
      "[ 0.77913778 -0.81935112  0.81188003  0.41120405  0.40738414  0.30022621\n",
      "  0.25672315  0.90441666  0.20479527  0.10894806]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_multi_armed_bandit_algorithms():\n",
    "    # Define the Multi-Armed Bandit environment\n",
    "    k_arms = 10\n",
    "    reward_distributions = [(np.random.rand(), 1) for _ in range(k_arms)]\n",
    "    \n",
    "    env = MultiArmedBanditEnv(k_arms, reward_distributions)\n",
    "\n",
    "    # Test Epsilon-Greedy Policy\n",
    "    print(\"Testing Epsilon-Greedy Policy...\")\n",
    "    q_values = epsilon_greedy_bandit(env)\n",
    "    print(\"Estimated Q-Values (Epsilon-Greedy):\")\n",
    "    print(q_values)\n",
    "    print()\n",
    "\n",
    "    # Test UCB Algorithm\n",
    "    print(\"Testing UCB Algorithm...\")\n",
    "    q_values = ucb_bandit(env)\n",
    "    print(\"Estimated Q-Values (UCB):\")\n",
    "    print(q_values)\n",
    "    print()\n",
    "\n",
    "# Run the test function\n",
    "test_multi_armed_bandit_algorithms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be30fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Testing Grid World Algorithms =====\n",
      "Testing Value Iteration...\n",
      "Optimal Policy (Value Iteration):\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "Value Table (Value Iteration):\n",
      "[[-99.99990137 -99.99990137 -99.99990137 -99.99990137 -99.99990137]\n",
      " [-99.99990137 -99.99990235 -99.99990235 -99.99990235 -99.99990235]\n",
      " [-99.99990235 -99.99990235 -99.99990235 -99.99990235 -99.99990235]\n",
      " [-99.99990235 -99.99990235 -99.99990235 -99.99990235 -99.99990235]\n",
      " [-99.99990235 -99.99990235 -99.99990235 -99.99990235 -99.99990235]]\n",
      "\n",
      "Testing Policy Iteration...\n",
      "Optimal Policy (Policy Iteration):\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "Value Table (Policy Iteration):\n",
      "[[-99.9999908  -99.9999908  -99.99999089 -99.99999089 -99.99999089]\n",
      " [-99.99999089 -99.99999089 -99.99999089 -99.99999089 -99.99999089]\n",
      " [-99.99999089 -99.99999089 -99.99999089 -99.99999089 -99.99999089]\n",
      " [-99.99999089 -99.99999089 -99.99999089 -99.99999089 -99.99999089]\n",
      " [-99.99999089 -99.99999089 -99.99999089 -99.99999089 -99.99999089]]\n",
      "\n",
      "Testing Q-Learning...\n",
      "Optimal Policy (Q-Learning):\n",
      "[[3 3 3 1 1]\n",
      " [1 0 3 1 1]\n",
      " [1 1 0 3 1]\n",
      " [3 1 1 0 1]\n",
      " [3 3 3 3 0]]\n",
      "Q-Table (Q-Learning):\n",
      "[[[-0.29559261 -3.45957156  0.90951571  2.52718827]\n",
      "  [ 1.61540522 -9.74968445  0.8109585   3.56281643]\n",
      "  [ 3.38664414  2.83508752  1.38250912  4.60890549]\n",
      "  [ 3.08838542  5.6655611   2.5458004   3.26153783]\n",
      "  [-0.39304654  6.12621174 -0.88194519 -0.89640839]]\n",
      "\n",
      " [[-3.1318478  -1.80629081 -3.22110752 -3.439     ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.91824451 -4.68559    -2.71        5.50678107]\n",
      "  [ 4.29907024  6.73289     3.02264174  6.07939865]\n",
      "  [ 0.48440787  7.80383531  1.14753147  0.75057585]]\n",
      "\n",
      " [[-2.36036533  0.35741037 -2.2682046  -2.16228711]\n",
      "  [-1.9        -0.6485071  -1.64227846 -2.71      ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 5.13700787 -8.49905365 -8.78423345  7.811     ]\n",
      "  [ 4.60399128  8.9         6.42518613  6.66977764]]\n",
      "\n",
      " [[-1.46164586 -1.45379675 -1.32617727  2.88444528]\n",
      "  [-0.96534008  5.30257212 -1.03009465 -0.85439962]\n",
      "  [-1.9         1.52825899 -0.61794978 -1.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 7.23078673 10.         -7.94108868  7.71364494]]\n",
      "\n",
      " [[-1.09448606 -0.99551198 -0.99215165 -0.75214429]\n",
      "  [-0.72299276 -0.52860333 -0.65426647  7.23581754]\n",
      "  [-0.36160674  1.49500115  0.31914475  8.73577246]\n",
      "  [-3.439      -0.1        -0.1108801   9.97534965]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "\n",
      "===== Testing Multi-Armed Bandit Algorithms =====\n",
      "Testing Epsilon-Greedy Policy...\n",
      "Estimated Q-Values (Epsilon-Greedy):\n",
      "[ 0.62003365  0.7169139  -0.26847858  0.03800157  0.40965003  0.66659644\n",
      "  0.69784858  0.76740997  0.08780223  0.68265251]\n",
      "\n",
      "Testing UCB Algorithm...\n",
      "Estimated Q-Values (UCB):\n",
      "[ 0.72380187  0.88454795 -0.15372887  0.62545238 -0.05652322  0.5897204\n",
      "  0.52169669  0.80277224  0.74914883  0.32576928]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"===== Testing Grid World Algorithms =====\")\n",
    "    test_grid_world_algorithms()\n",
    "    \n",
    "    print(\"===== Testing Multi-Armed Bandit Algorithms =====\")\n",
    "    test_multi_armed_bandit_algorithms()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62783c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
