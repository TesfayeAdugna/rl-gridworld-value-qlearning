{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9924e57d",
   "metadata": {},
   "source": [
    "# Grid World Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56861217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "[[ 3.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 2, Reward: -1\n",
      "[[ 1.  3.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 3, Reward: -1\n",
      "[[ 3.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 2, Reward: -1\n",
      "[[ 1.  3.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 3, Reward: -1\n",
      "[[ 1.  0.  3.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 3, Reward: -1\n",
      "[[ 1.  0.  0.  3.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 3, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  3.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 1, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  3.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 1, Reward: -1\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  3.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "Action: 1, Reward: -10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, grid_size, start, goal, obstacles):\n",
    "        super(GridWorldEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles\n",
    "        self.action_space = spaces.Discrete(4)  # Four actions: up, down, left, right\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(grid_size[0]),\n",
    "            spaces.Discrete(grid_size[1])\n",
    "        ))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = list(self.start)\n",
    "        return tuple(self.agent_pos)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Define actions: 0=up, 1=down, 2=left, 3=right\n",
    "        if action == 0 and self.agent_pos[0] > 0:  # Up\n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 1 and self.agent_pos[0] < self.grid_size[0] - 1:  # Down\n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 2 and self.agent_pos[1] > 0:  # Left\n",
    "            self.agent_pos[1] -= 1\n",
    "        elif action == 3 and self.agent_pos[1] < self.grid_size[1] - 1:  # Right\n",
    "            self.agent_pos[1] += 1\n",
    "        \n",
    "        reward = -1\n",
    "        done = False\n",
    "        if tuple(self.agent_pos) == self.goal:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif tuple(self.agent_pos) in self.obstacles:\n",
    "            reward = -10\n",
    "            done = True\n",
    "\n",
    "        return tuple(self.agent_pos), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.zeros(self.grid_size)\n",
    "        grid[self.start] = 1  # Start position\n",
    "        grid[self.goal] = 2  # Goal position\n",
    "        for obstacle in self.obstacles:\n",
    "            grid[obstacle] = -1  # Obstacle positions\n",
    "        grid[tuple(self.agent_pos)] = 3  # Agent position\n",
    "        print(grid)\n",
    "\n",
    "# Example usage\n",
    "grid_size = (5, 5)\n",
    "start = (0, 0)\n",
    "goal = (4, 4)\n",
    "obstacles = [(1, 1), (2, 2), (3, 3)]\n",
    "\n",
    "env = GridWorldEnv(grid_size, start, goal, obstacles)\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7bb84e",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d069fec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 6, Reward: 0.7870618088156315\n",
      "Action: 0, Reward: -0.4257115169570683\n",
      "Action: 1, Reward: -0.10371139520153574\n",
      "Action: 3, Reward: 0.5196974347441244\n",
      "Action: 1, Reward: 0.16188072254602276\n",
      "Action: 8, Reward: 2.801921680050778\n",
      "Action: 7, Reward: 0.956507470864909\n",
      "Action: 8, Reward: 2.590806644376442\n",
      "Action: 3, Reward: -0.49223398354003145\n",
      "Action: 6, Reward: -1.3373291643326914\n",
      "Action: 8, Reward: 1.5950496692073695\n",
      "Action: 8, Reward: 3.960706642120981\n",
      "Action: 2, Reward: -1.4349237603999578\n",
      "Action: 5, Reward: -0.06361534895716092\n",
      "Action: 8, Reward: 0.7325531852865608\n",
      "Action: 2, Reward: 0.9140492613457103\n",
      "Action: 7, Reward: -0.8051568387495164\n",
      "Action: 1, Reward: 0.37778958277793123\n",
      "Action: 5, Reward: 0.7707816863161439\n",
      "Action: 3, Reward: 1.548818412281353\n",
      "Action: 2, Reward: 0.6157867483337305\n",
      "Action: 4, Reward: 1.484606924893538\n",
      "Action: 2, Reward: -0.34466077849897925\n",
      "Action: 9, Reward: 0.14590453350785074\n",
      "Action: 1, Reward: -0.7321127020637822\n",
      "Action: 5, Reward: 1.7011852781425287\n",
      "Action: 1, Reward: 0.16326776920853214\n",
      "Action: 1, Reward: -0.32089193404251737\n",
      "Action: 9, Reward: 1.7297847419019399\n",
      "Action: 8, Reward: 0.08193884951853325\n",
      "Action: 0, Reward: -0.11918979187116285\n",
      "Action: 5, Reward: -0.6398704563805773\n",
      "Action: 1, Reward: 1.8066787574835408\n",
      "Action: 1, Reward: -1.1455802660753993\n",
      "Action: 7, Reward: -1.084580765056692\n",
      "Action: 1, Reward: -0.016885262152650765\n",
      "Action: 0, Reward: -1.5293133718842826\n",
      "Action: 4, Reward: 1.4101047373052369\n",
      "Action: 2, Reward: -1.1579454176917254\n",
      "Action: 2, Reward: -1.015414284033139\n",
      "Action: 8, Reward: 2.5294037773885645\n",
      "Action: 0, Reward: -1.0997751821552544\n",
      "Action: 9, Reward: 1.167820216939186\n",
      "Action: 4, Reward: 1.565572986470364\n",
      "Action: 7, Reward: 0.9910401627463554\n",
      "Action: 7, Reward: 1.6242620338405587\n",
      "Action: 9, Reward: 0.02940217907740353\n",
      "Action: 8, Reward: -0.7566445533840482\n",
      "Action: 9, Reward: 0.2407392255267579\n",
      "Action: 6, Reward: -0.10773603219026878\n",
      "Action: 8, Reward: 1.761778605230162\n",
      "Action: 3, Reward: -1.7022852550809562\n",
      "Action: 9, Reward: -1.9202431468335615\n",
      "Action: 2, Reward: -1.6916418901840569\n",
      "Action: 6, Reward: 0.5247490128012531\n",
      "Action: 8, Reward: 0.2921029883797216\n",
      "Action: 4, Reward: -0.45880570382106256\n",
      "Action: 5, Reward: 1.4491102795132669\n",
      "Action: 0, Reward: 0.49020126586131446\n",
      "Action: 7, Reward: 2.0004195658702817\n",
      "Action: 1, Reward: 0.40783851210080285\n",
      "Action: 8, Reward: 0.12151929045187004\n",
      "Action: 7, Reward: 0.03729515461298383\n",
      "Action: 6, Reward: 1.2031007003502403\n",
      "Action: 8, Reward: -0.06008674993304186\n",
      "Action: 8, Reward: 0.047873962299193495\n",
      "Action: 7, Reward: 0.9179779881823389\n",
      "Action: 5, Reward: -0.2534999819110546\n",
      "Action: 6, Reward: 0.9743485797459773\n",
      "Action: 2, Reward: 1.288481412619044\n",
      "Action: 6, Reward: 0.34872091919356885\n",
      "Action: 1, Reward: -0.35041142087672983\n",
      "Action: 8, Reward: 0.16114903494428123\n",
      "Action: 1, Reward: 0.6243930149508095\n",
      "Action: 3, Reward: 0.4351794484789085\n",
      "Action: 7, Reward: 0.8336457727728737\n",
      "Action: 8, Reward: -0.3357152828709773\n",
      "Action: 7, Reward: 1.4420387341003504\n",
      "Action: 0, Reward: 0.11666354411223134\n",
      "Action: 8, Reward: 1.0082022425922266\n",
      "Action: 3, Reward: -0.43223639208922404\n",
      "Action: 1, Reward: 1.324400902139041\n",
      "Action: 1, Reward: -0.355633501190559\n",
      "Action: 9, Reward: -0.10896364898339\n",
      "Action: 6, Reward: 0.29583983638754735\n",
      "Action: 3, Reward: 2.0786474355720945\n",
      "Action: 1, Reward: 0.7028533635633076\n",
      "Action: 6, Reward: -0.2646932442579366\n",
      "Action: 9, Reward: -2.668818426592411\n",
      "Action: 8, Reward: 1.080583338917093\n",
      "Action: 6, Reward: -0.35701958430451813\n",
      "Action: 9, Reward: 2.255890937896031\n",
      "Action: 1, Reward: -2.0854688803389574\n",
      "Action: 6, Reward: -1.6559576807689076\n",
      "Action: 4, Reward: 2.07468461012014\n",
      "Action: 5, Reward: 0.3109427080147936\n",
      "Action: 4, Reward: 1.4427146148023526\n",
      "Action: 6, Reward: 0.2577216304425877\n",
      "Action: 0, Reward: 0.18251911519737096\n",
      "Action: 6, Reward: 1.164168439403807\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiArmedBanditEnv(gym.Env):\n",
    "    def __init__(self, k_arms, reward_distributions):\n",
    "        super(MultiArmedBanditEnv, self).__init__()\n",
    "        self.k_arms = k_arms\n",
    "        self.reward_distributions = reward_distributions\n",
    "        self.action_space = spaces.Discrete(k_arms)\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        return 0  # Single-state problem\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = np.random.normal(self.reward_distributions[action][0], self.reward_distributions[action][1])\n",
    "        return 0, reward, False, {}  # Single-state, so always return state 0\n",
    "\n",
    "# Example usage\n",
    "k_arms = 10\n",
    "reward_distributions = [(np.random.rand(), 1) for _ in range(k_arms)]\n",
    "\n",
    "env = MultiArmedBanditEnv(k_arms, reward_distributions)\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c29e6d",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ad3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.observation_space.n):\n",
    "            v = value_table[state]\n",
    "            q_values = [sum([prob * (reward + gamma * value_table[next_state])\n",
    "                             for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                        for action in range(env.action_space.n)]\n",
    "            value_table[state] = max(q_values)\n",
    "            delta = max(delta, abs(v - value_table[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    for state in range(env.observation_space.n):\n",
    "        q_values = [sum([prob * (reward + gamma * value_table[next_state])\n",
    "                         for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                    for action in range(env.action_space.n)]\n",
    "        policy[state] = np.argmax(q_values)\n",
    "\n",
    "    return policy, value_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42932d",
   "metadata": {},
   "source": [
    "# Policy Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae09bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    policy = np.random.choice(env.action_space.n, env.observation_space.n)\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "\n",
    "    def one_step_lookahead(state, value_table):\n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                action_values[action] += prob * (reward + gamma * value_table[next_state])\n",
    "        return action_values\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(env.observation_space.n):\n",
    "                v = value_table[state]\n",
    "                value_table[state] = sum([prob * (reward + gamma * value_table[next_state])\n",
    "                                          for prob, next_state, reward, _ in env.P[state][policy[state]]])\n",
    "                delta = max(delta, abs(v - value_table[state]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "        for state in range(env.observation_space.n):\n",
    "            chosen_action = policy[state]\n",
    "            action_values = one_step_lookahead(state, value_table)\n",
    "            best_action = np.argmax(action_values)\n",
    "            if chosen_action != best_action:\n",
    "                policy_stable = False\n",
    "            policy[state] = best_action\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, value_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c5501",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfe6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=1000):\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def choose_action(state):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            return np.argmax(q_table[state, :])\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action])\n",
    "            state = next_state\n",
    "\n",
    "    policy = np.argmax(q_table, axis=1)\n",
    "    return policy, q_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b8303",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy Policy for Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fa93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_bandit(env, alpha=0.1, epsilon=0.1, episodes=1000):\n",
    "    q_values = np.zeros(env.action_space.n)\n",
    "    action_counts = np.zeros(env.action_space.n)\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        action_counts[action] += 1\n",
    "        q_values[action] += (reward - q_values[action]) / action_counts[action]\n",
    "\n",
    "    return q_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e37ae9",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound (UCB) Algorithm for Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76617428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_bandit(env, alpha=0.1, episodes=1000):\n",
    "    q_values = np.zeros(env.action_space.n)\n",
    "    action_counts = np.zeros(env.action_space.n)\n",
    "    total_steps = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        total_steps += 1\n",
    "        ucb_values = q_values + np.sqrt(2 * np.log(total_steps) / (action_counts + 1e-5))\n",
    "        action = np.argmax(ucb_values)\n",
    "\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        action_counts[action] += 1\n",
    "        q_values[action] += (reward - q_values[action]) / action_counts[action]\n",
    "\n",
    "    return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245e934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
